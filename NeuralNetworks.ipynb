{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neuron:\n",
    "    \"\"\"Neuron in neural network\n",
    "    \n",
    "    __init__(num_in,bias)\n",
    "        num_in = number of features to input to neuron (creates the number of weights on the neuron)\n",
    "        bias = True if a bias term should be included for w*x+b \n",
    "               False if no bias term should be included for w*x+b\n",
    "               default=True\n",
    "           \n",
    "\n",
    "    forward(x)\n",
    "        x = input to the neuron for w*x + b\n",
    "    \n",
    "    \n",
    "    backward(gradient_l)\n",
    "        gradient_l = the upstream gradient for backpropogation\n",
    "        gradient_x = the local gradient of input x\n",
    "        gradient_w = the local gradient of the neuron weights w\n",
    "        \n",
    "        return the local gradients * the upstream gradient\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self,num_in,bias=True):\n",
    "        self.gradient_x = None\n",
    "        self.gradient_w = None\n",
    "        \n",
    "        if bias:\n",
    "            num_in = num_in + 1\n",
    "            \n",
    "        self.weights = np.random.uniform(-1,1,size=(num_in,1))\n",
    "        self.x = None\n",
    "        self.bias=bias\n",
    "    \n",
    "    def forward(self,x):\n",
    "        if self.bias == True:\n",
    "            x = np.concatenate((x,[[1]]),axis=1)\n",
    "            \n",
    "        self.x = x\n",
    "        return np.matmul(x,self.weights)\n",
    "                                \n",
    "    def backward(self,gradient_l=1):\n",
    "        if self.x is not None:\n",
    "            self.gradient_x = self.weights.transpose()*gradient_l\n",
    "            self.gradient_w = self.x.transpose()*gradient_l\n",
    "            return self.gradient_x,self.gradient_w\n",
    "        else:\n",
    "            return None          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLu:\n",
    "    \"\"\"ReLu Activation function\n",
    "    \n",
    "    forward(x)\n",
    "    x = the input to sigmoid function max(0,x)\n",
    "        \n",
    "    backward(gradient_l)\n",
    "        gradient_l = the upstream gradient for backpropogation\n",
    "        self.gradient = the local gradient of the ReLu activation function\n",
    "        \n",
    "        return the local gradients * the upstream gradient\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.gradient = None\n",
    "        self.x = None\n",
    "    \n",
    "    def forward(self,x):\n",
    "        self.x = x\n",
    "        if x > 0:\n",
    "            return x\n",
    "        else:\n",
    "            return 0\n",
    "    \n",
    "    def backward(self,gradient_l=1):\n",
    "        if self.x is not None:\n",
    "            if self.x > 0:\n",
    "                self.gradient = gradient_l\n",
    "                return gradient_l\n",
    "            else:\n",
    "                self.gradient = sys.float_info.epsilon\n",
    "                return np.array([[sys.float_info.epsilon]])   \n",
    "        else:\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sigmoid:\n",
    "    \"\"\"Sigmoid Activation function\n",
    "    \n",
    "    forward(x)\n",
    "        x = the input to sigmoid function f(x) = 1 / (1+exp(-x))\n",
    "        \n",
    "    backward(gradient_l)\n",
    "        gradient_l = the upstream gradient for backpropogation\n",
    "        self.gradient = the local gradient of the Sigmoid activation function\n",
    "        \n",
    "        return the local gradients * the upstream gradient\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.gradient=None\n",
    "        self.x = None\n",
    "        \n",
    "    def forward(self,x):\n",
    "        self.x = x\n",
    "        return 1/(1+np.exp(-x))\n",
    "    \n",
    "    def backward(self,gradient_l=1):\n",
    "        if self.x is not None:\n",
    "            self.gradient = 1/(1+np.exp(-self.x))*(1-1/(1+np.exp(-self.x)))*gradient_l\n",
    "            return self.gradient\n",
    "        else:\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MSE:\n",
    "    \"\"\"\n",
    "    Binary Cross Entropy function\n",
    "    \n",
    "    forward(y_hat,y)\n",
    "        y_hat = the predicted classification probability 0-1\n",
    "        y = the training example actual label 0 or 1\n",
    "        binary cross entropy = -(y*ln(y_hat) + (1-y)*ln(1-y_hat))\n",
    "        \n",
    "    backward(gradient_l)\n",
    "        gradient_l = the upstream gradient for backpropogation\n",
    "        self.gradient = the local gradient of the MSE\n",
    "        \n",
    "        return the local gradients * the upstream gradient\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.gradient = None\n",
    "        self.y = None\n",
    "        self.y_hat = None\n",
    "        \n",
    "    def forward(self,y_hat,y):\n",
    "        self.y_hat = y_hat\n",
    "        self.y = y\n",
    "        return (y_hat-y)**2\n",
    "    \n",
    "    def backward(self,gradient_l=1):\n",
    "        if self.y is not None and self.y_hat is not None:\n",
    "            self.gradient = -2 * (self.y-self.y_hat)\n",
    "            return self.gradient\n",
    "        else:\n",
    "            return None\n",
    "        \n",
    "    def __str__(self):\n",
    "        return \"loss\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BCE:\n",
    "    \"\"\"Binary Cross Entropy function\n",
    "    \n",
    "    forward(y_hat,y)\n",
    "        y_hat = the predicted classification probability 0-1\n",
    "        y = the training example actual label 0 or 1\n",
    "        binary cross entropy = -(y*ln(y_hat) + (1-y)*ln(1-y_hat))\n",
    "        \n",
    "    backward(gradient_l)\n",
    "        gradient_l = the upstream gradient for backpropogation\n",
    "        self.gradient = the local gradient of the BCE \n",
    "        \n",
    "        return the local gradients * the upstream gradient\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.gradient = None\n",
    "        self.y = None\n",
    "        self.y_hat = None\n",
    "    \n",
    "    def forward(self,y_hat,y):\n",
    "        self.y_hat = y_hat\n",
    "        self.y = y\n",
    "        return -(y*np.log(max(sys.float_info.epsilon,y_hat-sys.float_info.epsilon)) + \\\n",
    "                 (1-y)*np.log(1-max(sys.float_info.epsilon,y_hat-sys.float_info.epsilon)))\n",
    "    \n",
    "    def backward(self,gradient_l=1):\n",
    "        if self.y is not None and self.y_hat is not None:\n",
    "            self.gradient = -self.y*1/(max(sys.float_info.epsilon,self.y_hat)) + \\\n",
    "                              1/max(sys.float_info.epsilon,1-self.y_hat)*(1-self.y)\n",
    "            return self.gradient\n",
    "        else:\n",
    "            return None\n",
    "        \n",
    "    def __str__(self):\n",
    "        return \"loss\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class neural_layer:\n",
    "    \"\"\"Neural Network layer\n",
    "    \n",
    "    __init__(input_,output_,bias=True)\n",
    "        input_ = the number of features in the training example\n",
    "        output_ = the number of neurons to use in the neural network layer\n",
    "        bias = True if a bias term should be included for w*x+b \n",
    "               False if no bias term should be included for w*x+b\n",
    "               default=True\n",
    "    \n",
    "    forward(x):\n",
    "        x = input to the neuron for W*x + B\n",
    "        \n",
    "    backward(gradient_l):\n",
    "        gradient_l = the upstream gradient for backpropogation\n",
    "        gradients has gradient_x and gradient_w for each neuron in the neural network layer\n",
    "        \n",
    "        return the local gradients * the upstream gradient\n",
    "    \"\"\"\n",
    "    def __init__(self,input_= 1,output_= 1,bias=True):\n",
    "        self.output_size = output_\n",
    "        self.input_size = input_\n",
    "        self.neurons = [Neuron(input_,bias) for _ in range(output_)]\n",
    "        \n",
    "    def forward(self,x):\n",
    "        outputs = [neuron.forward(x) for neuron in self.neurons]\n",
    "        return np.concatenate(outputs,1)\n",
    "    \n",
    "    def backward(self,gradient_l=None):\n",
    "        if gradient_l is None:\n",
    "            return None\n",
    "        \n",
    "        weight_gradients = []\n",
    "        input_gradient = 0\n",
    "        gradient_l = gradient_l.squeeze(0)       \n",
    "        \n",
    "        for i,neuron in enumerate(self.neurons):\n",
    "            temp_gradient = neuron.backward(gradient_l[i])\n",
    "            weight_gradients.append(temp_gradient[1])\n",
    "            input_gradient = input_gradient + temp_gradient[0]\n",
    "            \n",
    "        gradients = (input_gradient,weight_gradients)\n",
    "                    \n",
    "        return gradients\n",
    "    \n",
    "    def __str__(self):\n",
    "        return \"neuron\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class action_layer:\n",
    "    \"\"\"Activation layer\n",
    "    \n",
    "    __init__(output_,a_type=\"ReLu\")\n",
    "        output_ = the number of activation units to use in the neural network layer\n",
    "        a_type = the type of activation unit you want to use: \"ReLu\" or \"Sigmoid\"\n",
    "    \n",
    "    forward(x):\n",
    "        x = input to the activation layer\n",
    "        return the activation evaluation\n",
    "        \n",
    "    backward(gradient_l):\n",
    "        gradient_l = the upstream gradient for backpropogation\n",
    "        gradients has the local gradients for each activation unit\n",
    "        \n",
    "        return the local gradients * the upstream gradient\n",
    "    \"\"\"\n",
    "    def __init__(self,output_= 1,a_type=\"ReLu\"):\n",
    "        \n",
    "        self.output_size = output_\n",
    "        \n",
    "        if a_type == \"ReLu\":\n",
    "            self.activations = [ReLu() for _ in range(output_)]\n",
    "        if a_type == \"Sigmoid\":\n",
    "            self.activations = [Sigmoid() for _ in range(output_)]\n",
    "        \n",
    "    def forward(self,x):\n",
    "        output = []\n",
    "        x = x.squeeze(0)\n",
    "        for i,activation in enumerate(self.activations):\n",
    "            output.append(activation.forward(x[i]))\n",
    "        return np.expand_dims(output,0)\n",
    "    \n",
    "    def backward(self,gradient_l=None):\n",
    "        if gradient_l is None:\n",
    "            return None\n",
    "        \n",
    "        gradients = []\n",
    "        gradient_l = gradient_l.squeeze(0)     \n",
    "        \n",
    "        for i,activation in enumerate(self.activations):\n",
    "            gradients.append(activation.backward(gradient_l[i]))\n",
    "            \n",
    "        return np.expand_dims(gradients,0)\n",
    "    \n",
    "    def __str__(self):\n",
    "        return \"activation\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class neural_network:\n",
    "    \"\"\"Neural Network\n",
    "    \n",
    "    __init__(model=[])\n",
    "        model = a list [] of all the neural network layer objects and activation layer objects. The forward and backward execution\n",
    "                is sequential in regards to the order of the list passed in to model\n",
    "    \n",
    "    forward(x)\n",
    "        x = the input training example to the neural network which will pass through all the layers of neural network and activation\n",
    "            layers to classify or regress\n",
    "    \n",
    "    backward(gradient_l)\n",
    "        gradient_l = the upstream gradient from the loss function to the neural network\n",
    "\n",
    "        return all the gradients calculated throughout all the layers  \n",
    "        \n",
    "    model_weights()\n",
    "        return a list with all the model weights for each neural network layer with neurons\n",
    "    \"\"\"\n",
    "    def __init__(self,model=[]):\n",
    "        self.model = model\n",
    "        self.gradients = []\n",
    "        \n",
    "    def forward(self,x):\n",
    "        for obj in self.model:\n",
    "            x = obj.forward(x)    \n",
    "        return x\n",
    "    \n",
    "    def backward(self,gradient_l=None):\n",
    "        self.gradients = []\n",
    "        for obj in self.model[::-1]:\n",
    "            if str(obj) == \"activation\":\n",
    "                tup = (\"activation\",obj.backward(gradient_l))\n",
    "                self.gradients.append(tup)\n",
    "                gradient_l = tup[1]\n",
    "            \n",
    "            elif str(obj) == \"neuron\":\n",
    "                tup = (\"neuron\",obj.backward(gradient_l))\n",
    "                self.gradients.append(tup)\n",
    "                \n",
    "                if gradient_l is None:\n",
    "                    gradient_l = None\n",
    "                    continue\n",
    "                \n",
    "                gradient_l = tup[1][0]\n",
    "\n",
    "        self.gradients = self.gradients[::-1]\n",
    "        return self.gradients\n",
    "    \n",
    "    def get_weights(self):\n",
    "        weights = []\n",
    "        layer_num = 0\n",
    "        for obj in self.model:\n",
    "            if str(obj) == 'neuron':\n",
    "                layer_weights = []\n",
    "                for neuron in obj.neurons:\n",
    "                    layer_weights.append(neuron.weights)\n",
    "                weights.append(('layer'+str(layer_num),layer_weights))\n",
    "                layer_num += 1\n",
    "        return weights\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class optimizer:\n",
    "    \n",
    "    def __init__(self,model = None,alpha=0.05):\n",
    "        self.model = model\n",
    "        self.loss_fn = loss_fn\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def step(self):\n",
    "        if model is not None:\n",
    "            for obj,grad in zip(self.model.model,self.model.gradients):\n",
    "                if grad[0] == 'neuron':\n",
    "                    for n,grad_update in zip(obj.neurons,grad[1][1]):\n",
    "                        n.weights = n.weights - self.alpha*grad_update"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Classification Dataset (use the breast cancer dataset)\n",
    "* Download and use the breast cancer dataset with sklearn\n",
    "* standard normalize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "data = load_breast_cancer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data['data']\n",
    "X = X - np.mean(X,0)\n",
    "X = X / np.std(X,0)\n",
    "Y = data['target']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attempt to do logistic regression proof of concept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "neuron = Neuron(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "activation = Sigmoid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = BCE()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0: loss = [[0.14836266]]\n",
      "epoch 1: loss = [[0.11604855]]\n",
      "epoch 2: loss = [[0.1898477]]\n",
      "epoch 3: loss = [[0.11932667]]\n",
      "epoch 4: loss = [[0.13259908]]\n",
      "epoch 5: loss = [[0.1476856]]\n",
      "epoch 6: loss = [[0.17790457]]\n",
      "epoch 7: loss = [[0.11786576]]\n",
      "epoch 8: loss = [[0.12634756]]\n",
      "epoch 9: loss = [[0.13704256]]\n"
     ]
    }
   ],
   "source": [
    "alpha = 0.5\n",
    "print_every = 1\n",
    "for i in range(10):\n",
    "    mean_loss = []\n",
    "    for x,y in zip(X,Y):\n",
    "        x = np.expand_dims(x,0)\n",
    "\n",
    "        # forward pass\n",
    "        output = neuron.forward(x)\n",
    "        a = activation.forward(output)\n",
    "        \n",
    "        loss = loss_fn.forward(a,y)\n",
    "        mean_loss.append(loss)\n",
    "\n",
    "        # backward pass\n",
    "        loss_grad = loss_fn.backward()\n",
    "        a_grad = activation.backward(loss_grad)\n",
    "        _,weight_grad = neuron.backward(a_grad)\n",
    "\n",
    "        # update model\n",
    "        neuron.weights = neuron.weights - alpha*weight_grad\n",
    "        \n",
    "    if i % print_every == 0:\n",
    "        print(\"epoch {}: loss = {}\".format(i,np.mean(mean_loss)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predictions = []\n",
    "for x,y in zip(X,Y):\n",
    "    x = np.expand_dims(x,0)\n",
    "\n",
    "    # forward pass\n",
    "    output = neuron.forward(x)\n",
    "    a = activation.forward(output)\n",
    "    \n",
    "    y_predictions.append(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_pred = np.round(np.array(y_predictions).squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9789103690685413\n"
     ]
    }
   ],
   "source": [
    "accuracy = np.mean(y_pred == Y)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network Classification proof of concept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = [neural_layer(30,10),action_layer(10,\"ReLu\"),neural_layer(10,5),action_layer(5,\"ReLu\"),neural_layer(5,1),action_layer(a_type='Sigmoid')]\n",
    "nn = neural_network(model)\n",
    "loss_fn = BCE()\n",
    "\n",
    "alpha = 0.05\n",
    "\n",
    "optimize = optimizer(nn,alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0: loss = 0.16108593979583855\n",
      "epoch 5: loss = [[0.03731087]]\n",
      "epoch 10: loss = [[0.02520344]]\n",
      "epoch 15: loss = [[0.0253507]]\n",
      "epoch 20: loss = [[0.01709716]]\n",
      "epoch 25: loss = [[0.01182088]]\n",
      "epoch 30: loss = [[0.01205751]]\n",
      "epoch 35: loss = [[0.0095369]]\n",
      "epoch 40: loss = [[0.0049514]]\n",
      "epoch 45: loss = [[0.00405529]]\n"
     ]
    }
   ],
   "source": [
    "print_every = 5\n",
    "for i in range(50):\n",
    "    mean_loss = []\n",
    "    for x,y in zip(X,Y):\n",
    "        x = np.expand_dims(x,0)\n",
    "\n",
    "        # forward pass\n",
    "        output = nn.forward(x)\n",
    "        loss = loss_fn.forward(output,y)\n",
    "        mean_loss.append(loss)\n",
    "\n",
    "        # backward pass\n",
    "        loss_back = loss_fn.backward()\n",
    "        nn_back = nn.backward(loss_back)\n",
    "\n",
    "        # update weights based on gradient\n",
    "        optimize.step()\n",
    "    if i % print_every == 0:\n",
    "        print(\"epoch {}: loss = {}\".format(i,np.mean(mean_loss)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predictions = []\n",
    "for x,y in zip(X,Y):\n",
    "    x = np.expand_dims(x,0)\n",
    "\n",
    "    # forward pass\n",
    "    output = nn.forward(x)\n",
    "    \n",
    "    y_predictions.append(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = np.round(np.array(y_predictions).squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9982425307557118\n"
     ]
    }
   ],
   "source": [
    "accuracy = np.mean(y_pred == Y)\n",
    "print(accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
