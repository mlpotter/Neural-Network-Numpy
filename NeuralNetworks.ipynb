{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neuron:\n",
    "    \"\"\"Neuron in neural network\n",
    "    \n",
    "    __init__(num_in,bias)\n",
    "        num_in = number of features to input to neuron (creates the number of weights on the neuron)\n",
    "        bias = True if a bias term should be included for w*x+b \n",
    "               False if no bias term should be included for w*x+b\n",
    "               default=True\n",
    "           \n",
    "\n",
    "    forward(x)\n",
    "        x = input to the neuron for w*x + b\n",
    "    \n",
    "    \n",
    "    backward(gradient_l)\n",
    "        gradient_l = the upstream gradient for backpropogation\n",
    "        gradient_x = the local gradient of input x\n",
    "        gradient_w = the local gradient of the neuron weights w\n",
    "        \n",
    "        return the local gradients * the upstream gradient\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self,num_in,bias=True):\n",
    "        self.gradient_x = None\n",
    "        self.gradient_w = None\n",
    "        \n",
    "        if bias:\n",
    "            num_in = num_in + 1\n",
    "            \n",
    "        self.weights = np.random.uniform(-1,1,size=(num_in,1))\n",
    "        self.x = None\n",
    "        self.bias=bias\n",
    "    \n",
    "    def forward(self,x):\n",
    "        if self.bias == True:\n",
    "            x = np.concatenate((x,[[1]]),axis=1)\n",
    "            \n",
    "        self.x = x\n",
    "        return np.matmul(x,self.weights)\n",
    "                                \n",
    "    def backward(self,gradient_l=1):\n",
    "        if self.x is not None:\n",
    "            self.gradient_x = self.weights.transpose()*gradient_l\n",
    "            self.gradient_w = self.x.transpose()*gradient_l\n",
    "            return self.gradient_x,self.gradient_w\n",
    "        else:\n",
    "            return None          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLu:\n",
    "    \"\"\"ReLu Activation function\n",
    "    \n",
    "    forward(x)\n",
    "    x = the input to sigmoid function max(0,x)\n",
    "        \n",
    "    backward(gradient_l)\n",
    "        gradient_l = the upstream gradient for backpropogation\n",
    "        self.gradient = the local gradient of the Sigmoid activation function\n",
    "        \n",
    "        return the local gradients * the upstream gradient\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.gradient = None\n",
    "        self.x = None\n",
    "    \n",
    "    def forward(self,x):\n",
    "        self.x = x\n",
    "        if x > 0:\n",
    "            return x\n",
    "        else:\n",
    "            return 0\n",
    "    \n",
    "    def backward(self,gradient_l=1):\n",
    "        if self.x is not None:\n",
    "            if self.x > 0:\n",
    "                self.gradient = gradient_l\n",
    "                return gradient_l\n",
    "            else:\n",
    "                self.gradient = sys.float_info.epsilon\n",
    "                return np.array([[sys.float_info.epsilon]])   \n",
    "        else:\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sigmoid:\n",
    "    \"\"\"Sigmoid Activation function\n",
    "    \n",
    "    forward(x)\n",
    "        x = the input to sigmoid function f(x) = 1 / (1+exp(-x))\n",
    "        \n",
    "    backward(gradient_l)\n",
    "        gradient_l = the upstream gradient for backpropogation\n",
    "        self.gradient = the local gradient of the Sigmoid activation function\n",
    "        \n",
    "        return the local gradients * the upstream gradient\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.gradient=None\n",
    "        self.x = None\n",
    "        \n",
    "    def forward(self,x):\n",
    "        self.x = x\n",
    "        return 1/(1+np.exp(-x))\n",
    "    \n",
    "    def backward(self,gradient_l=1):\n",
    "        if self.x is not None:\n",
    "            self.gradient = 1/(1+np.exp(-self.x))*(1-1/(1+np.exp(-self.x)))*gradient_l\n",
    "            return self.gradient\n",
    "        else:\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BCE:\n",
    "    \"\"\"Binary Cross Entropy function\n",
    "    \n",
    "    forward(y_hat,y)\n",
    "        y_hat = the predicted classification probability 0-1\n",
    "        y = the training example actual label 0 or 1\n",
    "        binary cross entropy = -(y*ln(y_hat) + (1-y)*ln(1-y_hat))\n",
    "        \n",
    "    backward(gradient_l)\n",
    "        gradient_l = the upstream gradient for backpropogation\n",
    "        self.gradient = the local gradient of the ReLu activation function\n",
    "        \n",
    "        return the local gradients * the upstream gradient\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.gradient = None\n",
    "        self.y = None\n",
    "        self.y_hat = None\n",
    "    \n",
    "    def forward(self,y_hat,y):\n",
    "        self.y_hat = y_hat\n",
    "        self.y = y\n",
    "        return -(y*np.log(max(sys.float_info.epsilon,y_hat-sys.float_info.epsilon)) + \\\n",
    "                 (1-y)*np.log(1-max(sys.float_info.epsilon,y_hat-sys.float_info.epsilon)))\n",
    "    \n",
    "    def backward(self,gradient_l=1):\n",
    "        if self.y is not None and self.y_hat is not None:\n",
    "            self.gradient = -self.y*1/(max(sys.float_info.epsilon,self.y_hat)) + \\\n",
    "                              1/max(sys.float_info.epsilon,1-self.y_hat)*(1-self.y)\n",
    "            return self.gradient\n",
    "        else:\n",
    "            return None\n",
    "        \n",
    "    def __str__(self):\n",
    "        return \"loss\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class neural_layer:\n",
    "    \"\"\"Neural Network layer\n",
    "    \n",
    "    __init__(input_,output_,bias=True)\n",
    "        input_ = the number of features in the training example\n",
    "        output_ = the number of neurons to use in the neural network layer\n",
    "        bias = True if a bias term should be included for w*x+b \n",
    "               False if no bias term should be included for w*x+b\n",
    "               default=True\n",
    "    \n",
    "    forward(x):\n",
    "        x = input to the neuron for W*x + B\n",
    "        \n",
    "    backward(gradient_l):\n",
    "        gradient_l = the upstream gradient for backpropogation\n",
    "        gradients has gradient_x and gradient_w for each neuron in the neural network layer\n",
    "        \n",
    "        return the local gradients * the upstream gradient\n",
    "    \"\"\"\n",
    "    def __init__(self,input_= 1,output_= 1,bias=True):\n",
    "        self.output_size = output_\n",
    "        self.input_size = input_\n",
    "        self.neurons = [Neuron(input_,bias) for _ in range(output_)]\n",
    "        \n",
    "    def forward(self,x):\n",
    "        outputs = [neuron.forward(x) for neuron in self.neurons]\n",
    "        return np.concatenate(outputs,1)\n",
    "    \n",
    "    def backward(self,gradient_l=None):\n",
    "        if gradient_l is None:\n",
    "            return None\n",
    "        \n",
    "        gradients = []\n",
    "        gradient_l = gradient_l.squeeze(0)       \n",
    "        \n",
    "        for i,neuron in enumerate(self.neurons):\n",
    "            gradients.append(neuron.backward(gradient_l[i]))\n",
    "            \n",
    "        return gradients\n",
    "    \n",
    "    def __str__(self):\n",
    "        return \"neuron\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class action_layer:\n",
    "    \"\"\"Activation layer\n",
    "    \n",
    "    __init__(output_,a_type=\"ReLu\")\n",
    "        output_ = the number of activation units to use in the neural network layer\n",
    "        a_type = the type of activation unit you want to use: \"ReLu\" or \"Sigmoid\"\n",
    "    \n",
    "    forward(x):\n",
    "        x = input to the activation layer\n",
    "        return the activation evaluation\n",
    "        \n",
    "    backward(gradient_l):\n",
    "        gradient_l = the upstream gradient for backpropogation\n",
    "        gradients has the local gradients for each activation unit\n",
    "        \n",
    "        return the local gradients * the upstream gradient\n",
    "    \"\"\"\n",
    "    def __init__(self,output_= 1,a_type=\"ReLu\"):\n",
    "        \n",
    "        self.output_size = output_\n",
    "        \n",
    "        if a_type == \"ReLu\":\n",
    "            self.activations = [ReLu() for _ in range(output_)]\n",
    "        if a_type == \"Sigmoid\":\n",
    "            self.activations = [Sigmoid() for _ in range(output_)]\n",
    "        \n",
    "    def forward(self,x):\n",
    "        output = []\n",
    "        x = x.squeeze(0)\n",
    "        for i,activation in enumerate(self.activations):\n",
    "            output.append(activation.forward(x[i]))\n",
    "        return np.expand_dims(output,0)\n",
    "    \n",
    "    def backward(self,gradient_l=None):\n",
    "        if gradient_l is None:\n",
    "            return None\n",
    "        \n",
    "        gradients = []\n",
    "        gradient_l = gradient_l.squeeze(1)     \n",
    "        \n",
    "        for i,activation in enumerate(self.activations):\n",
    "            gradients.append(activation.backward(gradient_l[i]))\n",
    "            \n",
    "        return np.expand_dims(gradients,0)\n",
    "    \n",
    "    def __str__(self):\n",
    "        return \"activation\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class neural_network:\n",
    "    \"\"\"Neural Network\n",
    "    \n",
    "    __init__(model=[])\n",
    "        model = a list [] of all the neural network layer objects and activation layer objects. The forward and backward execution\n",
    "                is sequential in regards to the order of the list passed in to model\n",
    "    \n",
    "    forward(x)\n",
    "        x = the input training example to the neural network which will pass through all the layers of neural network and activation\n",
    "            layers to classify or regress\n",
    "    \n",
    "    backward(gradient_l)\n",
    "        gradient_l = the upstream gradient from the loss function to the neural network\n",
    "\n",
    "        return all the gradients calculated throughout all the layers  \n",
    "        \n",
    "    model_weights()\n",
    "        return a list with all the model weights for each neural network layer with neurons\n",
    "    \"\"\"\n",
    "    def __init__(self,model=[]):\n",
    "        self.model = model\n",
    "        self.gradients = []\n",
    "        \n",
    "    def forward(self,x):\n",
    "        for obj in self.model:\n",
    "            x = obj.forward(x)    \n",
    "        return x\n",
    "    \n",
    "    def backward(self,gradient_l=None):\n",
    "        self.gradients = []\n",
    "        for obj in self.model[::-1]:\n",
    "            if str(obj) == \"activation\":\n",
    "                tup = (\"activation\",obj.backward(gradient_l))\n",
    "                self.gradients.append(tup)\n",
    "                gradient_l = tup[1]\n",
    "            \n",
    "            elif str(obj) == \"neuron\":\n",
    "                tup = (\"neuron\",obj.backward(gradient_l))\n",
    "                self.gradients.append(tup)\n",
    "                \n",
    "                if gradient_l is None:\n",
    "                    gradient_l = None\n",
    "                    continue\n",
    "                \n",
    "                gradient_l = 0\n",
    "                for grad in tup[1]:\n",
    "                    gradient_l += grad[1]  \n",
    "\n",
    "        self.gradients = self.gradients[::-1]\n",
    "    \n",
    "    def model_weights(self):\n",
    "        weights = []\n",
    "        layer_num = 0\n",
    "        for obj in self.model:\n",
    "            if str(obj) == 'neuron':\n",
    "                layer_weights = []\n",
    "                for neuron in obj.neurons:\n",
    "                    layer_weights.append(neuron.weights)\n",
    "                weights.append(('layer'+str(layer_num),layer_weights))\n",
    "                layer_num += 1\n",
    "        return weights\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class optimizer:\n",
    "    \n",
    "    def __init__(self,model = None,alpha=0.05):\n",
    "        self.model = model\n",
    "        self.loss_fn = loss_fn\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def step(self):\n",
    "        if model is not None:\n",
    "            for obj,grad in zip(self.model.model,nn_back):\n",
    "                if grad[0] == 'neuron':\n",
    "                    for n,grad_update in zip(obj.neurons,grad[1]):\n",
    "                        n.weights = n.weights - self.alpha*grad_update[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Dataset (use the breast cancer dataset)\n",
    "* Download and use the breast cancer dataset with sklearn\n",
    "* standard normalize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "data = load_breast_cancer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data['data']\n",
    "X = X - np.mean(X,0)\n",
    "X = X / np.std(X,0)\n",
    "Y = data['target']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attempt to do logistic regression proof of concept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "neuron = Neuron(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "activation = Sigmoid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = BCE()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [[0.17052478]]\n",
      "1 [[0.16366847]]\n",
      "2 [[0.11125718]]\n",
      "3 [[0.16984805]]\n",
      "4 [[0.13401785]]\n",
      "5 [[0.10970913]]\n",
      "6 [[0.12862981]]\n",
      "7 [[0.12040971]]\n",
      "8 [[0.17409925]]\n",
      "9 [[0.11903616]]\n"
     ]
    }
   ],
   "source": [
    "alpha = 0.5\n",
    "\n",
    "for i in range(10):\n",
    "    mean_loss = []\n",
    "    for x,y in zip(X,Y):\n",
    "        x = np.expand_dims(x,0)\n",
    "\n",
    "        # forward pass\n",
    "        output = neuron.forward(x)\n",
    "        a = activation.forward(output)\n",
    "        \n",
    "        loss = loss_fn.forward(a,y)\n",
    "        mean_loss.append(loss)\n",
    "\n",
    "        # backward pass\n",
    "        loss_grad = loss_fn.backward()\n",
    "        a_grad = activation.backward(loss_grad)\n",
    "        _,weight_grad = neuron.backward(a_grad)\n",
    "\n",
    "        # update model\n",
    "        neuron.weights = neuron.weights - alpha*weight_grad\n",
    "    print(i,np.mean(mean_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predictions = []\n",
    "for x,y in zip(X,Y):\n",
    "    x = np.expand_dims(x,0)\n",
    "\n",
    "    # forward pass\n",
    "    output = neuron.forward(x)\n",
    "    a = activation.forward(output)\n",
    "    \n",
    "    y_predictions.append(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_pred = np.round(np.array(y_predictions).squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9859402460456942\n"
     ]
    }
   ],
   "source": [
    "accuracy = np.mean(y_pred == Y)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network proof of concept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = [neural_layer(30,10),action_layer(10,\"Sigmoid\"),neural_layer(10,5),action_layer(5,\"Sigmoid\"),neural_layer(5,1),action_layer(a_type='Sigmoid')]\n",
    "nn = neural_network(model)\n",
    "loss_fn = BCE()\n",
    "\n",
    "alpha = 0.5\n",
    "\n",
    "optimize = optimizer(nn,alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.2530462874459362\n",
      "1 0.08891745442087008\n",
      "2 0.07043116081939194\n",
      "3 0.0647242423658837\n",
      "4 0.06083244871364005\n",
      "5 0.05796712124094374\n",
      "6 0.05611251580649331\n",
      "7 0.054350388370757445\n",
      "8 0.052499552355417735\n",
      "9 0.050702046808938576\n",
      "10 0.04906293381776169\n",
      "11 0.04759922421410999\n",
      "12 0.04629470935634199\n",
      "13 0.045121090099461435\n",
      "14 0.044053576750476386\n",
      "15 0.04307192791023223\n",
      "16 0.042159654357381775\n",
      "17 0.0413034869201217\n",
      "18 0.040493183853086846\n",
      "19 0.03972112417470696\n",
      "20 0.03898164874940537\n",
      "21 0.03827034939253695\n",
      "22 0.03758350011149982\n",
      "23 0.03691773641056787\n",
      "24 0.03626999120515198\n",
      "25 0.03563761654367768\n",
      "26 0.035018586465298464\n",
      "27 0.03441166364168312\n",
      "28 0.033816335075692816\n",
      "29 0.03323217305902614\n",
      "30 0.0326575375624006\n",
      "31 0.032088863121644176\n",
      "32 0.03152219804627751\n",
      "33 0.030955355709326136\n",
      "34 0.03038778209451058\n",
      "35 0.029819216825911533\n",
      "36 0.02924912073079108\n",
      "37 0.028676797267660672\n",
      "38 0.02810159065498993\n",
      "39 0.027522856568046077\n",
      "40 0.02693956964647803\n",
      "41 0.026349528993835098\n",
      "42 0.02574827712601645\n",
      "43 0.02512804273446442\n",
      "44 0.024477304620707005\n",
      "45 0.023782092428925292\n",
      "46 0.02303037633206683\n",
      "47 0.022218962902686417\n",
      "48 0.02135815558269778\n",
      "49 0.02046953648966923\n"
     ]
    }
   ],
   "source": [
    "for i in range(50):\n",
    "    mean_loss = []\n",
    "    for x,y in zip(X,Y):\n",
    "        x = np.expand_dims(x,0)\n",
    "\n",
    "        # forward pass\n",
    "        output = nn.forward(x)\n",
    "        loss = loss_fn.forward(output,y)\n",
    "        mean_loss.append(loss)\n",
    "\n",
    "        # backward pass\n",
    "        loss_back = loss_fn.backward()\n",
    "        nn_back = nn.backward(loss_back)\n",
    "\n",
    "        # update weights based on gradient\n",
    "        optimize.step()\n",
    "\n",
    "    print(i,np.mean(mean_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predictions = []\n",
    "for x,y in zip(X,Y):\n",
    "    x = np.expand_dims(x,0)\n",
    "\n",
    "    # forward pass\n",
    "    output = nn.forward(x)\n",
    "    \n",
    "    y_predictions.append(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = np.round(np.array(y_predictions).squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9507908611599297\n"
     ]
    }
   ],
   "source": [
    "accuracy = np.mean(y_pred == Y)\n",
    "print(accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
