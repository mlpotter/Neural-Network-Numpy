{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neuron:\n",
    "    \"\"\"Neuron in neural network\n",
    "    \n",
    "    __init__(num_in,bias)\n",
    "        num_in = number of features to input to neuron (creates the number of weights on the neuron)\n",
    "        bias = True if a bias term should be included for w*x+b \n",
    "               False if no bias term should be included for w*x+b\n",
    "               default=True\n",
    "           \n",
    "\n",
    "    forward(x)\n",
    "        x = input to the neuron for w*x + b\n",
    "    \n",
    "    \n",
    "    backward(gradient_l)\n",
    "        gradient_l = the upstream gradient for backpropogation\n",
    "        gradient_x = the local gradient of input x\n",
    "        gradient_w = the local gradient of the neuron weights w\n",
    "        \n",
    "        return the local gradients * the upstream gradient\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self,num_in,bias=True):\n",
    "        self.gradient_x = None\n",
    "        self.gradient_w = None\n",
    "        \n",
    "        if bias:\n",
    "            num_in = num_in + 1\n",
    "            \n",
    "        self.weights = np.random.uniform(-1,1,size=(num_in,1))\n",
    "        self.x = None\n",
    "        self.bias=bias\n",
    "    \n",
    "    def forward(self,x):\n",
    "        if self.bias == True:\n",
    "            x = np.concatenate((x,[[1]]),axis=1)\n",
    "            \n",
    "        self.x = x\n",
    "        return np.matmul(x,self.weights)\n",
    "                                \n",
    "    def backward(self,gradient_l=1):\n",
    "        if self.x is not None:\n",
    "            self.gradient_x = self.weights.transpose()*gradient_l\n",
    "            self.gradient_w = self.x.transpose()*gradient_l\n",
    "            return self.gradient_x,self.gradient_w\n",
    "        else:\n",
    "            return None          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLu:\n",
    "    \"\"\"ReLu Activation function\n",
    "    \n",
    "    forward(x)\n",
    "    x = the input to sigmoid function max(0,x)\n",
    "        \n",
    "    backward(gradient_l)\n",
    "        gradient_l = the upstream gradient for backpropogation\n",
    "        self.gradient = the local gradient of the Sigmoid activation function\n",
    "        \n",
    "        return the local gradients * the upstream gradient\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.gradient = None\n",
    "        self.x = None\n",
    "    \n",
    "    def forward(self,x):\n",
    "        self.x = x\n",
    "        if x > 0:\n",
    "            return x\n",
    "        else:\n",
    "            return 0\n",
    "    \n",
    "    def backward(self,gradient_l=1):\n",
    "        if self.x is not None:\n",
    "            if self.x > 0:\n",
    "                self.gradient = gradient_l\n",
    "                return gradient_l\n",
    "            else:\n",
    "                self.gradient = sys.float_info.epsilon\n",
    "                return np.array([[sys.float_info.epsilon]])   \n",
    "        else:\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sigmoid:\n",
    "    \"\"\"Sigmoid Activation function\n",
    "    \n",
    "    forward(x)\n",
    "        x = the input to sigmoid function f(x) = 1 / (1+exp(-x))\n",
    "        \n",
    "    backward(gradient_l)\n",
    "        gradient_l = the upstream gradient for backpropogation\n",
    "        self.gradient = the local gradient of the Sigmoid activation function\n",
    "        \n",
    "        return the local gradients * the upstream gradient\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.gradient=None\n",
    "        self.x = None\n",
    "        \n",
    "    def forward(self,x):\n",
    "        self.x = x\n",
    "        return 1/(1+np.exp(-x))\n",
    "    \n",
    "    def backward(self,gradient_l=1):\n",
    "        if self.x is not None:\n",
    "            self.gradient = 1/(1+np.exp(-self.x))*(1-1/(1+np.exp(-self.x)))*gradient_l\n",
    "            return self.gradient\n",
    "        else:\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BCE:\n",
    "    \"\"\"Binary Cross Entropy function\n",
    "    \n",
    "    forward(y_hat,y)\n",
    "        y_hat = the predicted classification probability 0-1\n",
    "        y = the training example actual label 0 or 1\n",
    "        binary cross entropy = -(y*ln(y_hat) + (1-y)*ln(1-y_hat))\n",
    "        \n",
    "    backward(gradient_l)\n",
    "        gradient_l = the upstream gradient for backpropogation\n",
    "        self.gradient = the local gradient of the ReLu activation function\n",
    "        \n",
    "        return the local gradients * the upstream gradient\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.gradient = None\n",
    "        self.y = None\n",
    "        self.y_hat = None\n",
    "    \n",
    "    def forward(self,y_hat,y):\n",
    "        self.y_hat = y_hat\n",
    "        self.y = y\n",
    "        return -(y*np.log(max(sys.float_info.epsilon,y_hat-sys.float_info.epsilon)) + \\\n",
    "                 (1-y)*np.log(1-max(sys.float_info.epsilon,y_hat-sys.float_info.epsilon)))\n",
    "    \n",
    "    def backward(self,gradient_l=1):\n",
    "        if self.y is not None and self.y_hat is not None:\n",
    "            self.gradient = -self.y*1/(max(sys.float_info.epsilon,self.y_hat)) + \\\n",
    "                              1/max(sys.float_info.epsilon,1-self.y_hat)*(1-self.y)\n",
    "            return self.gradient\n",
    "        else:\n",
    "            return None\n",
    "        \n",
    "    def __str__(self):\n",
    "        return \"loss\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class neural_layer:\n",
    "    \"\"\"Neural Network layer\n",
    "    \n",
    "    __init__(input_,output_,bias=True)\n",
    "        input_ = the number of features in the training example\n",
    "        output_ = the number of neurons to use in the neural network layer\n",
    "        bias = True if a bias term should be included for w*x+b \n",
    "               False if no bias term should be included for w*x+b\n",
    "               default=True\n",
    "    \n",
    "    forward(x):\n",
    "        x = input to the neuron for W*x + B\n",
    "        \n",
    "    backward(gradient_l):\n",
    "        gradient_l = the upstream gradient for backpropogation\n",
    "        gradients has gradient_x and gradient_w for each neuron in the neural network layer\n",
    "        \n",
    "        return the local gradients * the upstream gradient\n",
    "    \"\"\"\n",
    "    def __init__(self,input_= 1,output_= 1,bias=True):\n",
    "        self.output_size = output_\n",
    "        self.input_size = input_\n",
    "        self.neurons = [Neuron(input_,bias) for _ in range(output_)]\n",
    "        \n",
    "    def forward(self,x):\n",
    "        outputs = [neuron.forward(x) for neuron in self.neurons]\n",
    "        return np.concatenate(outputs,1)\n",
    "    \n",
    "    def backward(self,gradient_l=None):\n",
    "        if gradient_l is None:\n",
    "            return None\n",
    "        \n",
    "        gradients = []\n",
    "        gradient_l = gradient_l.squeeze(0)       \n",
    "        \n",
    "        for i,neuron in enumerate(self.neurons):\n",
    "            gradients.append(neuron.backward(gradient_l[i]))\n",
    "            \n",
    "        return gradients\n",
    "    \n",
    "    def __str__(self):\n",
    "        return \"neuron\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class action_layer:\n",
    "    \"\"\"Activation layer\n",
    "    \n",
    "    __init__(output_,a_type=\"ReLu\")\n",
    "        output_ = the number of activation units to use in the neural network layer\n",
    "        a_type = the type of activation unit you want to use: \"ReLu\" or \"Sigmoid\"\n",
    "    \n",
    "    forward(x):\n",
    "        x = input to the activation layer\n",
    "        return the activation evaluation\n",
    "        \n",
    "    backward(gradient_l):\n",
    "        gradient_l = the upstream gradient for backpropogation\n",
    "        gradients has the local gradients for each activation unit\n",
    "        \n",
    "        return the local gradients * the upstream gradient\n",
    "    \"\"\"\n",
    "    def __init__(self,output_= 1,a_type=\"ReLu\"):\n",
    "        \n",
    "        self.output_size = output_\n",
    "        \n",
    "        if a_type == \"ReLu\":\n",
    "            self.activations = [ReLu() for _ in range(output_)]\n",
    "        if a_type == \"Sigmoid\":\n",
    "            self.activations = [Sigmoid() for _ in range(output_)]\n",
    "        \n",
    "    def forward(self,x):\n",
    "        output = []\n",
    "        x = x.squeeze(0)\n",
    "        for i,activation in enumerate(self.activations):\n",
    "            output.append(activation.forward(x[i]))\n",
    "        return np.expand_dims(output,0)\n",
    "    \n",
    "    def backward(self,gradient_l=None):\n",
    "        if gradient_l is None:\n",
    "            return None\n",
    "        \n",
    "        gradients = []\n",
    "        gradient_l = gradient_l.squeeze(1)     \n",
    "        \n",
    "        for i,activation in enumerate(self.activations):\n",
    "            gradients.append(activation.backward(gradient_l[i]))\n",
    "            \n",
    "        return np.expand_dims(gradients,0)\n",
    "    \n",
    "    def __str__(self):\n",
    "        return \"activation\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class neural_network:\n",
    "    \"\"\"Neural Network\n",
    "    \n",
    "    __init__(model=[])\n",
    "        model = a list [] of all the neural network layer objects and activation layer objects. The forward and backward execution\n",
    "                is sequential in regards to the order of the list passed in to model\n",
    "    \n",
    "    forward(x)\n",
    "        x = the input training example to the neural network which will pass through all the layers of neural network and activation\n",
    "            layers to classify or regress\n",
    "    \n",
    "    backward(gradient_l)\n",
    "        gradient_l = the upstream gradient from the loss function to the neural network\n",
    "\n",
    "        return all the gradients calculated throughout all the layers  \n",
    "        \n",
    "    model_weights()\n",
    "        return a list with all the model weights for each neural network layer with neurons\n",
    "    \"\"\"\n",
    "    def __init__(self,model=[]):\n",
    "        self.model = model\n",
    "        \n",
    "    def forward(self,x):\n",
    "        for obj in self.model:\n",
    "            x = obj.forward(x)    \n",
    "        return x\n",
    "    \n",
    "    def backward(self,gradient_l=None):\n",
    "        gradients = []\n",
    "        for obj in self.model[::-1]:\n",
    "            if str(obj) == \"activation\":\n",
    "                tup = (\"activation\",obj.backward(gradient_l))\n",
    "                gradients.append(tup)\n",
    "                gradient_l = tup[1]\n",
    "            \n",
    "            elif str(obj) == \"neuron\":\n",
    "                tup = (\"neuron\",obj.backward(gradient_l))\n",
    "                gradients.append(tup)\n",
    "                \n",
    "                if gradient_l is None:\n",
    "                    gradient_l = None\n",
    "                    continue\n",
    "                \n",
    "                gradient_l = 0\n",
    "                for grad in tup[1]:\n",
    "                    gradient_l += grad[1]  \n",
    "\n",
    "        return gradients[::-1]\n",
    "    \n",
    "    def model_weights(self):\n",
    "        weights = []\n",
    "        layer_num = 0\n",
    "        for obj in self.model:\n",
    "            if str(obj) == 'neuron':\n",
    "                layer_weights = []\n",
    "                for neuron in obj.neurons:\n",
    "                    layer_weights.append(neuron.weights)\n",
    "                weights.append(('layer'+str(layer_num),layer_weights))\n",
    "                layer_num += 1\n",
    "        return weights\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Dataset (use the breast cancer dataset)\n",
    "* Download and use the breast cancer dataset with sklearn\n",
    "* standard normalize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "data = load_breast_cancer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data['data']\n",
    "X = X - np.mean(X,0)\n",
    "X = X / np.std(X,0)\n",
    "Y = data['target']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attempt to do logistic regression proof of concept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "neuron = Neuron(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "activation = Sigmoid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = BCE()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [[0.27279014]]\n",
      "1 [[0.14471549]]\n",
      "2 [[0.1184541]]\n",
      "3 [[0.11579671]]\n",
      "4 [[0.11725409]]\n",
      "5 [[0.17587143]]\n",
      "6 [[0.1077476]]\n",
      "7 [[0.13020292]]\n",
      "8 [[0.13110511]]\n",
      "9 [[0.16060833]]\n"
     ]
    }
   ],
   "source": [
    "alpha = 0.5\n",
    "\n",
    "for i in range(10):\n",
    "    mean_loss = []\n",
    "    for x,y in zip(X,Y):\n",
    "        x = np.expand_dims(x,0)\n",
    "\n",
    "        # forward pass\n",
    "        output = neuron.forward(x)\n",
    "        a = activation.forward(output)\n",
    "        \n",
    "        loss = loss_fn.forward(a,y)\n",
    "        mean_loss.append(loss)\n",
    "\n",
    "        # backward pass\n",
    "        loss_grad = loss_fn.backward()\n",
    "        a_grad = activation.backward(loss_grad)\n",
    "        _,weight_grad = neuron.backward(a_grad)\n",
    "\n",
    "        # update model\n",
    "        neuron.weights = neuron.weights - alpha*weight_grad\n",
    "    print(i,np.mean(mean_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predictions = []\n",
    "for x,y in zip(X,Y):\n",
    "    x = np.expand_dims(x,0)\n",
    "\n",
    "    # forward pass\n",
    "    output = neuron.forward(x)\n",
    "    a = activation.forward(output)\n",
    "    \n",
    "    y_predictions.append(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_pred = np.round(np.array(y_predictions).squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9859402460456942\n"
     ]
    }
   ],
   "source": [
    "accuracy = np.mean(y_pred == Y)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network proof of concept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = [neural_layer(30,10),action_layer(10,\"Sigmoid\"),neural_layer(10,5),action_layer(5,\"Sigmoid\"),neural_layer(5,1),action_layer(a_type='Sigmoid')]\n",
    "nn = neural_network(model)\n",
    "loss_fn = BCE()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.26362795187646115\n",
      "1 0.09586376753463155\n",
      "2 0.07805496866045593\n",
      "3 0.07478517027120367\n",
      "4 0.07149311158295918\n",
      "5 0.06995581517101132\n",
      "6 0.0642999881219297\n",
      "7 0.061424953164168794\n",
      "8 0.05737389693085057\n",
      "9 0.05507430677757546\n"
     ]
    }
   ],
   "source": [
    "alpha = 0.5\n",
    "\n",
    "for i in range(10):\n",
    "    mean_loss = []\n",
    "    for x,y in zip(X,Y):\n",
    "        x = np.expand_dims(x,0)\n",
    "\n",
    "        # forward pass\n",
    "        output = nn.forward(x)\n",
    "        loss = loss_fn.forward(output,y)\n",
    "        mean_loss.append(loss)\n",
    "\n",
    "        # backward pass\n",
    "        loss_back = loss_fn.backward()\n",
    "        nn_back = nn.backward(loss_back)\n",
    "\n",
    "        for obj,grad in zip(nn.model,nn_back):\n",
    "            if grad[0] == 'neuron':\n",
    "                for n,grad_update in zip(obj.neurons,grad[1]):\n",
    "                    n.weights = n.weights - alpha*grad_update[1]\n",
    "\n",
    "    print(i,np.mean(mean_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predictions = []\n",
    "for x,y in zip(X,Y):\n",
    "    x = np.expand_dims(x,0)\n",
    "\n",
    "    # forward pass\n",
    "    output = nn.forward(x)\n",
    "    \n",
    "    y_predictions.append(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = np.round(np.array(y_predictions).squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9771528998242531\n"
     ]
    }
   ],
   "source": [
    "accuracy = np.mean(y_pred == Y)\n",
    "print(accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
