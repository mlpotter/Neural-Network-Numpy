{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neuron:\n",
    "    \"\"\"Neuron in neural network\n",
    "    \n",
    "    __init__(num_in,bias)\n",
    "        num_in = number of features to input to neuron (creates the number of weights on the neuron)\n",
    "        bias = True if a bias term should be included for w*x+b \n",
    "               False if no bias term should be included for w*x+b\n",
    "               default=True\n",
    "           \n",
    "\n",
    "    forward(x)\n",
    "        x = input to the neuron for w*x + b\n",
    "    \n",
    "    \n",
    "    backward(gradient_l)\n",
    "        gradient_l = the upstream gradient for backpropogation\n",
    "        gradient_x = the local gradient of input x\n",
    "        gradient_w = the local gradient of the neuron weights w\n",
    "        \n",
    "        return the local gradients * the upstream gradient\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self,num_in,bias=True):\n",
    "        self.gradient_x = None\n",
    "        self.gradient_w = None\n",
    "        \n",
    "        if bias:\n",
    "            num_in = num_in + 1\n",
    "            \n",
    "        self.weights = np.random.uniform(-1,1,size=(num_in,1))\n",
    "        self.x = None\n",
    "        self.bias=bias\n",
    "    \n",
    "    def forward(self,x):\n",
    "        if self.bias == True:\n",
    "            x = np.concatenate((x,[[1]]),axis=1)\n",
    "            \n",
    "        self.x = x\n",
    "        return np.matmul(x,self.weights)\n",
    "                                \n",
    "    def backward(self,gradient_l=1):\n",
    "        if self.x is not None:\n",
    "            self.gradient_x = self.weights.transpose()*gradient_l\n",
    "            self.gradient_w = self.x.transpose()*gradient_l\n",
    "            return self.gradient_x,self.gradient_w\n",
    "        else:\n",
    "            return None          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLu:\n",
    "    \"\"\"ReLu Activation function\n",
    "    \n",
    "    forward(x)\n",
    "    x = the input to sigmoid function max(0,x)\n",
    "        \n",
    "    backward(gradient_l)\n",
    "        gradient_l = the upstream gradient for backpropogation\n",
    "        self.gradient = the local gradient of the Sigmoid activation function\n",
    "        \n",
    "        return the local gradients * the upstream gradient\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.gradient = None\n",
    "        self.x = None\n",
    "    \n",
    "    def forward(self,x):\n",
    "        self.x = x\n",
    "        if x > 0:\n",
    "            return x\n",
    "        else:\n",
    "            return 0\n",
    "    \n",
    "    def backward(self,gradient_l=1):\n",
    "        if self.x is not None:\n",
    "            if self.x > 0:\n",
    "                self.gradient = gradient_l\n",
    "                return gradient_l\n",
    "            else:\n",
    "                self.gradient = 0\n",
    "                return 0    \n",
    "        else:\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sigmoid:\n",
    "    \"\"\"Sigmoid Activation function\n",
    "    \n",
    "    forward(x)\n",
    "        x = the input to sigmoid function f(x) = 1 / (1+exp(-x))\n",
    "        \n",
    "    backward(gradient_l)\n",
    "        gradient_l = the upstream gradient for backpropogation\n",
    "        self.gradient = the local gradient of the Sigmoid activation function\n",
    "        \n",
    "        return the local gradients * the upstream gradient\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.gradient=None\n",
    "        self.x = None\n",
    "        \n",
    "    def forward(self,x):\n",
    "        self.x = x\n",
    "        return 1/(1+np.exp(-x))\n",
    "    \n",
    "    def backward(self,gradient_l=1):\n",
    "        if self.x is not None:\n",
    "            self.gradient = 1/(1+np.exp(-self.x))*(1-1/(1+np.exp(-self.x)))*gradient_l\n",
    "            return self.gradient\n",
    "        else:\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BCE:\n",
    "    \"\"\"Binary Cross Entropy function\n",
    "    \n",
    "    forward(y_hat,y)\n",
    "        y_hat = the predicted classification probability 0-1\n",
    "        y = the training example actual label 0 or 1\n",
    "        binary cross entropy = -(y*ln(y_hat) + (1-y)*ln(1-y_hat))\n",
    "        \n",
    "    backward(gradient_l)\n",
    "        gradient_l = the upstream gradient for backpropogation\n",
    "        self.gradient = the local gradient of the ReLu activation function\n",
    "        \n",
    "        return the local gradients * the upstream gradient\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.gradient = None\n",
    "        self.y = None\n",
    "        self.y_hat = None\n",
    "    \n",
    "    def forward(self,y_hat,y):\n",
    "        self.y_hat = y_hat\n",
    "        self.y = y\n",
    "        return -(y*np.log(max(sys.float_info.epsilon,y_hat-sys.float_info.epsilon)) + \\\n",
    "                 (1-y)*np.log(1-max(sys.float_info.epsilon,y_hat-sys.float_info.epsilon)))\n",
    "    \n",
    "    def backward(self,gradient_l=1):\n",
    "        if self.y is not None and self.y_hat is not None:\n",
    "            self.gradient = -self.y*1/(max(sys.float_info.epsilon,self.y_hat)) + \\\n",
    "                              1/(1-max(self.y_hat,sys.float_info.epsilon))*(1-self.y)\n",
    "            return self.gradient\n",
    "        else:\n",
    "            return None\n",
    "        \n",
    "    def __str__(self):\n",
    "        return \"loss\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class neural_layer:\n",
    "    \"\"\"Neural Network layer\n",
    "    \n",
    "    __init__(input_,output_,bias=True)\n",
    "        input_ = the number of features in the training example\n",
    "        output_ = the number of neurons to use in the neural network layer\n",
    "        bias = True if a bias term should be included for w*x+b \n",
    "               False if no bias term should be included for w*x+b\n",
    "               default=True\n",
    "    \n",
    "    forward(x):\n",
    "        x = input to the neuron for W*x + B\n",
    "        \n",
    "    backward(gradient_l):\n",
    "        gradient_l = the upstream gradient for backpropogation\n",
    "        gradients has gradient_x and gradient_w for each neuron in the neural network layer\n",
    "        \n",
    "        return the local gradients * the upstream gradient\n",
    "    \"\"\"\n",
    "    def __init__(self,input_= 1,output_= 1,bias=True):\n",
    "        self.output_size = output_\n",
    "        self.input_size = input_\n",
    "        self.neurons = [Neuron(input_,bias) for _ in range(output_)]\n",
    "        \n",
    "    def forward(self,x):\n",
    "        outputs = [neuron.forward(x) for neuron in self.neurons]\n",
    "        return np.concatenate(outputs,1)\n",
    "    \n",
    "    def backward(self,gradient_l=None):\n",
    "        if gradient_l is None or None in gradient_l:\n",
    "            return None\n",
    "        \n",
    "        gradients = []\n",
    "        gradient_l = gradient_l.squeeze(0)       \n",
    "        \n",
    "        for i,neuron in enumerate(self.neurons):\n",
    "            gradients.append(neuron.backward(gradient_l[i]))\n",
    "            \n",
    "        return gradients\n",
    "    \n",
    "    def __str__(self):\n",
    "        return \"neuron\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class action_layer:\n",
    "    \"\"\"Activation layer\n",
    "    \n",
    "    __init__(output_,a_type=\"ReLu\")\n",
    "        output_ = the number of activation units to use in the neural network layer\n",
    "        a_type = the type of activation unit you want to use: \"ReLu\" or \"Sigmoid\"\n",
    "    \n",
    "    forward(x):\n",
    "        x = input to the activation layer\n",
    "        return the activation evaluation\n",
    "        \n",
    "    backward(gradient_l):\n",
    "        gradient_l = the upstream gradient for backpropogation\n",
    "        gradients has the local gradients for each activation unit\n",
    "        \n",
    "        return the local gradients * the upstream gradient\n",
    "    \"\"\"\n",
    "    def __init__(self,output_= 1,a_type=\"ReLu\"):\n",
    "        \n",
    "        self.output_size = output_\n",
    "        \n",
    "        if a_type == \"ReLu\":\n",
    "            self.activations = [ReLu() for _ in range(output_)]\n",
    "        if a_type == \"Sigmoid\":\n",
    "            self.activations = [Sigmoid() for _ in range(output_)]\n",
    "        \n",
    "    def forward(self,x):\n",
    "        output = []\n",
    "        x = x.squeeze(0)\n",
    "        for i,activation in enumerate(self.activations):\n",
    "            output.append(activation.forward(x[i]))\n",
    "        return np.expand_dims(output,0)\n",
    "    \n",
    "    def backward(self,gradient_l=None):\n",
    "        if gradient_l is None or None in gradient_l:\n",
    "            return None\n",
    "        \n",
    "        gradients = []\n",
    "        gradient_l = gradient_l.squeeze(1)     \n",
    "        \n",
    "        for i,activation in enumerate(self.activations):\n",
    "            gradients.append(activation.backward(gradient_l[i]))\n",
    "            \n",
    "        return np.expand_dims(gradients,0)\n",
    "    \n",
    "    def __str__(self):\n",
    "        return \"activation\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class neural_network:\n",
    "    \"\"\"Neural Network\n",
    "    \n",
    "    __init__(model=[])\n",
    "        model = a list [] of all the neural network layer objects and activation layer objects. The forward and backward execution\n",
    "                is sequential in regards to the order of the list passed in to model\n",
    "    \n",
    "    forward(x)\n",
    "        x = the input training example to the neural network which will pass through all the layers of neural network and activation\n",
    "            layers to classify or regress\n",
    "    \n",
    "    backward(gradient_l)\n",
    "        gradient_l = the upstream gradient from the loss function to the neural network\n",
    "\n",
    "        return all the gradients calculated throughout all the layers  \n",
    "        \n",
    "    model_weights()\n",
    "        return a list with all the model weights for each neural network layer with neurons\n",
    "    \"\"\"\n",
    "    def __init__(self,model=[]):\n",
    "        self.model = model\n",
    "        \n",
    "    def forward(self,x):\n",
    "        for obj in self.model:\n",
    "            x = obj.forward(x)    \n",
    "        return x\n",
    "    \n",
    "    def backward(self,gradient_l=None):\n",
    "        gradients = []\n",
    "        for obj in self.model[::-1]:\n",
    "            if str(obj) == \"activation\":\n",
    "                tup = (\"activation\",obj.backward(gradient_l))\n",
    "                gradients.append(tup)\n",
    "                gradient_l = tup[1]\n",
    "            \n",
    "            elif str(obj) == \"neuron\":\n",
    "                tup = (\"neuron\",obj.backward(gradient_l))\n",
    "                gradients.append(tup)\n",
    "                \n",
    "                if gradient_l is None or None in gradient_l:\n",
    "                    gradient_l = None\n",
    "                    continue\n",
    "                \n",
    "                gradient_l = 0\n",
    "                for grad in tup[1]:\n",
    "                    gradient_l += grad[1]  \n",
    "\n",
    "        return gradients[::-1]\n",
    "    \n",
    "    def model_weights(self):\n",
    "        weights = []\n",
    "        layer_num = 0\n",
    "        for obj in self.model:\n",
    "            if str(obj) == 'neuron':\n",
    "                layer_weights = []\n",
    "                for neuron in obj.neurons:\n",
    "                    layer_weights.append(neuron.weights)\n",
    "                weights.append(('layer'+str(layer_num),layer_weights))\n",
    "                layer_num += 1\n",
    "        return weights\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Dataset (use the breast cancer dataset)\n",
    "* Download and use the breast cancer dataset with sklearn\n",
    "* standard normalize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "data = load_breast_cancer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data['data']\n",
    "X = X - np.mean(X,0)\n",
    "X = X / np.std(X,0)\n",
    "Y = data['target']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attempt to do logistic regression proof of concept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "neuron = Neuron(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "activation = Sigmoid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = BCE()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [[0.16865002]]\n",
      "1 [[0.08160309]]\n",
      "2 [[0.06936675]]\n",
      "3 [[0.06491617]]\n",
      "4 [[0.06305983]]\n",
      "5 [[0.06230607]]\n",
      "6 [[0.06185728]]\n",
      "7 [[0.06104828]]\n",
      "8 [[0.06020596]]\n",
      "9 [[0.05954941]]\n",
      "10 [[0.05898587]]\n",
      "11 [[0.05845267]]\n",
      "12 [[0.0579555]]\n",
      "13 [[0.05749291]]\n",
      "14 [[0.05705958]]\n",
      "15 [[0.05665435]]\n",
      "16 [[0.05627402]]\n",
      "17 [[0.05591503]]\n",
      "18 [[0.05557539]]\n",
      "19 [[0.05525354]]\n",
      "20 [[0.05494786]]\n",
      "21 [[0.05465686]]\n",
      "22 [[0.05437928]]\n",
      "23 [[0.05411406]]\n",
      "24 [[0.05386022]]\n",
      "25 [[0.0536169]]\n",
      "26 [[0.05338332]]\n",
      "27 [[0.0531588]]\n",
      "28 [[0.05294272]]\n",
      "29 [[0.05273452]]\n",
      "30 [[0.0525337]]\n",
      "31 [[0.05233979]]\n",
      "32 [[0.05215238]]\n",
      "33 [[0.05197108]]\n",
      "34 [[0.05179554]]\n",
      "35 [[0.05162543]]\n",
      "36 [[0.05146045]]\n",
      "37 [[0.05130033]]\n",
      "38 [[0.05114481]]\n",
      "39 [[0.05099364]]\n",
      "40 [[0.05084661]]\n",
      "41 [[0.0507035]]\n",
      "42 [[0.05056411]]\n",
      "43 [[0.05042827]]\n",
      "44 [[0.05029581]]\n",
      "45 [[0.05016657]]\n",
      "46 [[0.05004038]]\n",
      "47 [[0.04991712]]\n",
      "48 [[0.04979665]]\n",
      "49 [[0.04967884]]\n",
      "50 [[0.04956358]]\n",
      "51 [[0.04945075]]\n",
      "52 [[0.04934025]]\n",
      "53 [[0.04923198]]\n",
      "54 [[0.04912585]]\n",
      "55 [[0.04902177]]\n",
      "56 [[0.04891965]]\n",
      "57 [[0.04881941]]\n",
      "58 [[0.04872098]]\n",
      "59 [[0.0486243]]\n",
      "60 [[0.04852928]]\n",
      "61 [[0.04843587]]\n",
      "62 [[0.048344]]\n",
      "63 [[0.04825363]]\n",
      "64 [[0.04816468]]\n",
      "65 [[0.04807712]]\n",
      "66 [[0.04799089]]\n",
      "67 [[0.04790595]]\n",
      "68 [[0.04782224]]\n",
      "69 [[0.04773973]]\n",
      "70 [[0.04765838]]\n",
      "71 [[0.04757814]]\n",
      "72 [[0.04749898]]\n",
      "73 [[0.04742087]]\n",
      "74 [[0.04734377]]\n",
      "75 [[0.04726765]]\n",
      "76 [[0.04719247]]\n",
      "77 [[0.04711821]]\n",
      "78 [[0.04704485]]\n",
      "79 [[0.04697234]]\n",
      "80 [[0.04690068]]\n",
      "81 [[0.04682982]]\n",
      "82 [[0.04675976]]\n",
      "83 [[0.04669046]]\n",
      "84 [[0.0466219]]\n",
      "85 [[0.04655407]]\n",
      "86 [[0.04648694]]\n",
      "87 [[0.0464205]]\n",
      "88 [[0.04635472]]\n",
      "89 [[0.0462896]]\n",
      "90 [[0.0462251]]\n",
      "91 [[0.04616122]]\n",
      "92 [[0.04609793]]\n",
      "93 [[0.04603524]]\n",
      "94 [[0.04597311]]\n",
      "95 [[0.04591154]]\n",
      "96 [[0.04585051]]\n",
      "97 [[0.04579001]]\n",
      "98 [[0.04573002]]\n",
      "99 [[0.04567054]]\n"
     ]
    }
   ],
   "source": [
    "alpha = 0.1\n",
    "\n",
    "for i in range(100):\n",
    "    mean_loss = []\n",
    "    for x,y in zip(X,Y):\n",
    "        x = np.expand_dims(x,0)\n",
    "\n",
    "        # forward pass\n",
    "        output = neuron.forward(x)\n",
    "        a = activation.forward(output)\n",
    "        \n",
    "        loss = loss_fn.forward(a,y)\n",
    "        mean_loss.append(loss)\n",
    "\n",
    "        # backward pass\n",
    "        loss_grad = loss_fn.backward()\n",
    "        a_grad = activation.backward(loss_grad)\n",
    "        _,weight_grad = neuron.backward(a_grad)\n",
    "\n",
    "        # update model\n",
    "        neuron.weights = neuron.weights - alpha*weight_grad\n",
    "    print(i,np.mean(mean_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predictions = []\n",
    "for x,y in zip(X,Y):\n",
    "    x = np.expand_dims(x,0)\n",
    "\n",
    "    # forward pass\n",
    "    output = neuron.forward(x)\n",
    "    a = activation.forward(output)\n",
    "    \n",
    "    y_predictions.append(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_pred = np.round(np.array(y_predictions).squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9876977152899824\n"
     ]
    }
   ],
   "source": [
    "accuracy = np.mean(y_pred == Y)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network proof of concept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = [neural_layer(30,10),action_layer(10,\"Sigmoid\"),neural_layer(10,5),action_layer(5,\"Sigmoid\"),neural_layer(5,1),action_layer(a_type='Sigmoid')]\n",
    "nn = neural_network(model)\n",
    "loss_fn = BCE()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.36971208512399917\n",
      "1 0.18032312460218217\n",
      "2 0.13214447475923818\n",
      "3 0.11090338191791328\n",
      "4 0.0984090582772083\n",
      "5 0.09028666308952955\n",
      "6 0.08486738104722792\n",
      "7 0.08109836729528654\n",
      "8 0.07833829509321606\n",
      "9 0.07620718558843136\n",
      "10 0.07447378024471511\n",
      "11 0.07303207543348252\n",
      "12 0.07182635868780293\n",
      "13 0.07073027170993157\n",
      "14 0.06959685013986586\n",
      "15 0.06842278908653776\n",
      "16 0.06722325210909366\n",
      "17 0.0660004335166812\n",
      "18 0.06478680597110413\n",
      "19 0.06367662078719795\n",
      "20 0.06271774608475764\n",
      "21 0.06187110259658513\n",
      "22 0.061096982424687564\n",
      "23 0.06037415168216306\n",
      "24 0.05969188936298923\n",
      "25 0.059044836020668366\n",
      "26 0.05843015203665773\n",
      "27 0.05784590689349415\n",
      "28 0.05729030756717159\n",
      "29 0.05676144751615699\n",
      "30 0.056257301893553456\n",
      "31 0.05577579713860038\n",
      "32 0.05531487807889012\n",
      "33 0.05487255148088472\n",
      "34 0.05444690873044744\n",
      "35 0.05403613699514415\n",
      "36 0.05363852730090537\n",
      "37 0.05325248366786549\n",
      "38 0.05287653237168449\n",
      "39 0.05250932732154328\n",
      "40 0.052149648341896504\n",
      "41 0.051796392832556895\n",
      "42 0.05144856468662236\n",
      "43 0.05110526511706202\n",
      "44 0.0507656883676288\n",
      "45 0.05042912273608676\n",
      "46 0.0500949551155362\n",
      "47 0.049762675884948857\n",
      "48 0.049431880964808175\n",
      "49 0.04910226975071031\n",
      "50 0.048773641056176265\n",
      "51 0.04844589229647365\n",
      "52 0.04811902804507911\n",
      "53 0.047793182599913026\n",
      "54 0.047468657744650034\n",
      "55 0.047145968150379795\n",
      "56 0.04682585906095022\n",
      "57 0.046509183700158094\n",
      "58 0.04619638452775217\n",
      "59 0.045886328363916734\n",
      "60 0.04557521210710527\n",
      "61 0.04525793075671612\n",
      "62 0.04493175054935202\n",
      "63 0.04459708934125549\n",
      "64 0.044255021685615965\n",
      "65 0.0439056342157128\n",
      "66 0.04354778143765095\n",
      "67 0.04317921461459212\n",
      "68 0.042796916244088754\n",
      "69 0.04239820865015848\n",
      "70 0.04198334302454093\n",
      "71 0.041558549186749276\n",
      "72 0.04113504499277964\n",
      "73 0.04072226493140405\n",
      "74 0.040323225938543386\n",
      "75 0.039936519150835154\n",
      "76 0.039559655017693277\n",
      "77 0.039190513283277086\n",
      "78 0.03882756314779745\n",
      "79 0.03846973918809676\n",
      "80 0.03811629298919308\n",
      "81 0.037766685065239405\n",
      "82 0.03742051456136248\n",
      "83 0.03707747441756042\n",
      "84 0.03673732229479414\n",
      "85 0.03639986115480926\n",
      "86 0.036064925890414705\n",
      "87 0.03573237390202766\n",
      "88 0.03540207836919758\n",
      "89 0.035073923443717445\n",
      "90 0.034747800858116684\n",
      "91 0.0344236075937603\n",
      "92 0.034101244340069406\n",
      "93 0.03378061453111135\n",
      "94 0.03346162378516304\n",
      "95 0.03314417960565666\n",
      "96 0.03282819123197544\n",
      "97 0.03251356955665501\n",
      "98 0.032200227051096736\n",
      "99 0.03188807766393193\n"
     ]
    }
   ],
   "source": [
    "alpha = 0.1\n",
    "\n",
    "for i in range(100):\n",
    "    mean_loss = []\n",
    "    for x,y in zip(X,Y):\n",
    "        x = np.expand_dims(x,0)\n",
    "\n",
    "        # forward pass\n",
    "        output = nn.forward(x)\n",
    "        loss = loss_fn.forward(output,y)\n",
    "        mean_loss.append(loss)\n",
    "\n",
    "        # backward pass\n",
    "        loss_back = loss_fn.backward()\n",
    "        nn_back = nn.backward(loss_back)\n",
    "\n",
    "        for obj,grad in zip(nn.model,nn_back):\n",
    "            if grad[0] == 'neuron':\n",
    "                for n,grad_update in zip(obj.neurons,grad[1]):\n",
    "                    n.weights = n.weights - alpha*grad_update[1]\n",
    "\n",
    "    print(i,np.mean(mean_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predictions = []\n",
    "for x,y in zip(X,Y):\n",
    "    x = np.expand_dims(x,0)\n",
    "\n",
    "    # forward pass\n",
    "    output = nn.forward(x)\n",
    "    \n",
    "    y_predictions.append(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = np.round(np.array(y_predictions).squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9894551845342706\n"
     ]
    }
   ],
   "source": [
    "accuracy = np.mean(y_pred == Y)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('layer0',\n",
       "  [array([[ 0.19286178],\n",
       "          [-0.30298063],\n",
       "          [-0.02019768],\n",
       "          [-0.50455462],\n",
       "          [ 0.0611002 ],\n",
       "          [ 0.76532093],\n",
       "          [ 0.33755652],\n",
       "          [-1.00696302],\n",
       "          [ 0.13815685],\n",
       "          [ 0.38612644],\n",
       "          [-0.88094713],\n",
       "          [-0.89439378],\n",
       "          [-0.70314072],\n",
       "          [-0.46138417],\n",
       "          [-0.07231421],\n",
       "          [ 0.79833636],\n",
       "          [-0.24130275],\n",
       "          [ 0.20561182],\n",
       "          [-0.26050891],\n",
       "          [-0.16832721],\n",
       "          [-0.82570205],\n",
       "          [-1.42379458],\n",
       "          [ 0.75051402],\n",
       "          [-0.00428219],\n",
       "          [-0.01661449],\n",
       "          [-0.37045314],\n",
       "          [-0.57274244],\n",
       "          [-0.64491736],\n",
       "          [-0.50055881],\n",
       "          [ 0.73570731],\n",
       "          [ 0.18612675]]),\n",
       "   array([[ 0.83852024],\n",
       "          [-0.37788092],\n",
       "          [-0.60634691],\n",
       "          [ 0.75953214],\n",
       "          [ 0.19792922],\n",
       "          [-0.24007013],\n",
       "          [-1.18534436],\n",
       "          [-0.81176259],\n",
       "          [-0.09586881],\n",
       "          [ 0.68190732],\n",
       "          [-0.94990641],\n",
       "          [ 1.2041912 ],\n",
       "          [-1.28186979],\n",
       "          [ 0.34666596],\n",
       "          [-0.60814855],\n",
       "          [ 0.92647238],\n",
       "          [ 0.08708164],\n",
       "          [ 0.58225239],\n",
       "          [ 0.00647255],\n",
       "          [-0.20229777],\n",
       "          [-1.28856898],\n",
       "          [-0.51107852],\n",
       "          [-0.80754345],\n",
       "          [ 0.2561787 ],\n",
       "          [-0.11206186],\n",
       "          [ 0.73929502],\n",
       "          [-0.62720387],\n",
       "          [-0.32578013],\n",
       "          [-0.35434627],\n",
       "          [-0.34324764],\n",
       "          [ 0.04816666]]),\n",
       "   array([[-0.16690793],\n",
       "          [ 0.88896574],\n",
       "          [ 0.45926651],\n",
       "          [ 0.12276359],\n",
       "          [ 0.0095801 ],\n",
       "          [ 0.97481424],\n",
       "          [ 0.39430869],\n",
       "          [ 0.16488142],\n",
       "          [ 0.42924905],\n",
       "          [ 0.45405007],\n",
       "          [ 0.15629527],\n",
       "          [ 0.55240877],\n",
       "          [ 0.02621642],\n",
       "          [-0.89388055],\n",
       "          [-1.26328206],\n",
       "          [ 1.38581008],\n",
       "          [ 0.77982039],\n",
       "          [-0.90993165],\n",
       "          [-0.56406818],\n",
       "          [ 0.80280229],\n",
       "          [ 0.16833129],\n",
       "          [ 0.27067684],\n",
       "          [ 0.19079518],\n",
       "          [-0.49739466],\n",
       "          [-0.9142886 ],\n",
       "          [ 0.14475011],\n",
       "          [ 0.33040519],\n",
       "          [ 0.41415297],\n",
       "          [-0.51736807],\n",
       "          [-0.68938684],\n",
       "          [-0.61923266]]),\n",
       "   array([[ 0.2445254 ],\n",
       "          [-1.37696726],\n",
       "          [-1.03655981],\n",
       "          [ 0.63657555],\n",
       "          [ 0.09593179],\n",
       "          [-0.31919941],\n",
       "          [ 0.94726004],\n",
       "          [-0.43427569],\n",
       "          [ 0.31349735],\n",
       "          [-0.27374421],\n",
       "          [-1.4751427 ],\n",
       "          [-0.48481163],\n",
       "          [-0.9120796 ],\n",
       "          [-0.0875811 ],\n",
       "          [ 1.0284116 ],\n",
       "          [ 1.31982857],\n",
       "          [-0.62334305],\n",
       "          [ 0.37862573],\n",
       "          [-0.39157778],\n",
       "          [ 1.09438566],\n",
       "          [ 0.04480732],\n",
       "          [-0.5051743 ],\n",
       "          [-0.7632951 ],\n",
       "          [ 0.17505716],\n",
       "          [-0.14274055],\n",
       "          [-0.51413672],\n",
       "          [-0.76674654],\n",
       "          [-1.01803463],\n",
       "          [-0.41793357],\n",
       "          [-0.44357238],\n",
       "          [-0.93599251]]),\n",
       "   array([[ 0.6668992 ],\n",
       "          [ 0.71967374],\n",
       "          [-0.50710441],\n",
       "          [-0.08064297],\n",
       "          [ 0.19174457],\n",
       "          [-0.888657  ],\n",
       "          [-0.10441673],\n",
       "          [-0.34484673],\n",
       "          [ 0.69871904],\n",
       "          [-0.78486956],\n",
       "          [-1.35116875],\n",
       "          [ 0.50681873],\n",
       "          [-1.16328909],\n",
       "          [-0.7842319 ],\n",
       "          [-0.44879868],\n",
       "          [ 0.38142678],\n",
       "          [ 0.3890019 ],\n",
       "          [-0.62927718],\n",
       "          [-0.37983705],\n",
       "          [-0.02842513],\n",
       "          [ 0.13063473],\n",
       "          [ 0.23825591],\n",
       "          [-0.31567441],\n",
       "          [ 0.08217467],\n",
       "          [ 0.26076515],\n",
       "          [-0.43308908],\n",
       "          [-0.82278097],\n",
       "          [-0.80188728],\n",
       "          [ 0.05417902],\n",
       "          [ 0.54411067],\n",
       "          [ 0.16267326]]),\n",
       "   array([[-0.41805046],\n",
       "          [ 0.71884161],\n",
       "          [-1.14892652],\n",
       "          [ 0.23414992],\n",
       "          [-0.63381265],\n",
       "          [ 0.08179361],\n",
       "          [-0.54952775],\n",
       "          [-0.49721429],\n",
       "          [ 0.475968  ],\n",
       "          [ 0.94517572],\n",
       "          [ 0.23054307],\n",
       "          [ 1.13879538],\n",
       "          [-0.26390526],\n",
       "          [ 0.08953203],\n",
       "          [-0.33486413],\n",
       "          [-0.62495431],\n",
       "          [ 0.40464317],\n",
       "          [ 0.59235266],\n",
       "          [ 0.08267603],\n",
       "          [ 0.26003511],\n",
       "          [ 0.10093545],\n",
       "          [-0.3764966 ],\n",
       "          [ 0.52359029],\n",
       "          [-1.20916302],\n",
       "          [-1.42580168],\n",
       "          [ 0.96653801],\n",
       "          [-1.2121662 ],\n",
       "          [-0.82403694],\n",
       "          [-0.84779856],\n",
       "          [ 0.93500618],\n",
       "          [-0.83417086]]),\n",
       "   array([[ 0.17898887],\n",
       "          [ 0.76687218],\n",
       "          [-1.40303324],\n",
       "          [-0.92862546],\n",
       "          [-0.27429868],\n",
       "          [ 0.87080447],\n",
       "          [ 0.17519115],\n",
       "          [-0.15379119],\n",
       "          [-0.67286697],\n",
       "          [ 0.7524009 ],\n",
       "          [-0.62379087],\n",
       "          [-0.14191115],\n",
       "          [ 0.04006506],\n",
       "          [-0.3363261 ],\n",
       "          [ 0.54164864],\n",
       "          [ 1.21362435],\n",
       "          [ 0.43915317],\n",
       "          [-1.03087599],\n",
       "          [ 0.71687593],\n",
       "          [ 0.38869781],\n",
       "          [-1.55395085],\n",
       "          [-0.25056161],\n",
       "          [ 0.16192739],\n",
       "          [ 0.35295442],\n",
       "          [-1.11358111],\n",
       "          [ 0.51850103],\n",
       "          [-0.52944854],\n",
       "          [-0.39093522],\n",
       "          [ 0.35630221],\n",
       "          [ 0.11429405],\n",
       "          [-0.29972237]]),\n",
       "   array([[ 0.71706558],\n",
       "          [ 0.77781026],\n",
       "          [ 0.53059216],\n",
       "          [-0.69707594],\n",
       "          [-0.53894985],\n",
       "          [ 0.25231514],\n",
       "          [-0.26178653],\n",
       "          [-0.22863381],\n",
       "          [ 0.22716144],\n",
       "          [-1.01579705],\n",
       "          [-0.86038336],\n",
       "          [-0.35599049],\n",
       "          [-1.20268642],\n",
       "          [-1.60277215],\n",
       "          [ 0.85615111],\n",
       "          [-0.3305377 ],\n",
       "          [ 0.67764491],\n",
       "          [-0.44547114],\n",
       "          [ 0.51507017],\n",
       "          [-0.63659537],\n",
       "          [-0.35053183],\n",
       "          [ 0.26089599],\n",
       "          [-1.15577317],\n",
       "          [-0.64815954],\n",
       "          [-0.24291457],\n",
       "          [ 0.31189974],\n",
       "          [-0.19696196],\n",
       "          [-0.39999386],\n",
       "          [-0.33838655],\n",
       "          [-0.45448715],\n",
       "          [ 0.76172719]]),\n",
       "   array([[-0.15367894],\n",
       "          [ 0.32761404],\n",
       "          [-0.06159095],\n",
       "          [-0.84833728],\n",
       "          [ 0.2772846 ],\n",
       "          [-1.07208565],\n",
       "          [-0.67346706],\n",
       "          [-0.08755924],\n",
       "          [ 0.70146194],\n",
       "          [-0.60236133],\n",
       "          [-1.55372644],\n",
       "          [-0.2795557 ],\n",
       "          [-0.59174094],\n",
       "          [ 0.2691869 ],\n",
       "          [-1.01492821],\n",
       "          [ 0.09356118],\n",
       "          [ 0.17154735],\n",
       "          [-0.53237514],\n",
       "          [ 0.13278289],\n",
       "          [ 0.17247369],\n",
       "          [ 0.02593358],\n",
       "          [ 0.3987931 ],\n",
       "          [-1.02817292],\n",
       "          [-1.25371238],\n",
       "          [-1.10793701],\n",
       "          [-0.35592115],\n",
       "          [-0.07049536],\n",
       "          [-0.93706708],\n",
       "          [ 0.41666086],\n",
       "          [ 0.23164479],\n",
       "          [ 0.16923339]]),\n",
       "   array([[ 0.14707734],\n",
       "          [-0.539055  ],\n",
       "          [-0.86413642],\n",
       "          [ 0.76748664],\n",
       "          [-0.62601096],\n",
       "          [-0.20708641],\n",
       "          [ 0.04202728],\n",
       "          [-0.00903733],\n",
       "          [ 0.3086118 ],\n",
       "          [ 0.16114792],\n",
       "          [ 0.3243343 ],\n",
       "          [ 1.34368634],\n",
       "          [-0.33202097],\n",
       "          [ 0.44367173],\n",
       "          [-1.03632562],\n",
       "          [ 0.8239211 ],\n",
       "          [-0.56319428],\n",
       "          [ 0.55944577],\n",
       "          [ 0.5988199 ],\n",
       "          [ 0.37699872],\n",
       "          [-1.15129991],\n",
       "          [-1.04036771],\n",
       "          [-0.86837584],\n",
       "          [-0.37089443],\n",
       "          [-0.37276824],\n",
       "          [-0.58588521],\n",
       "          [-0.48854736],\n",
       "          [-1.24694912],\n",
       "          [-1.30528905],\n",
       "          [ 0.28393874],\n",
       "          [ 0.81391726]])]),\n",
       " ('layer1',\n",
       "  [array([[ 0.86882623],\n",
       "          [ 1.46970011],\n",
       "          [-0.75739037],\n",
       "          [ 1.13604251],\n",
       "          [-0.15267284],\n",
       "          [ 0.28065911],\n",
       "          [ 0.58071478],\n",
       "          [-0.07746755],\n",
       "          [ 0.71425736],\n",
       "          [ 1.25718582],\n",
       "          [-1.75080342]]),\n",
       "   array([[ 1.27124811],\n",
       "          [ 0.95654253],\n",
       "          [ 0.32866982],\n",
       "          [ 0.72465072],\n",
       "          [-0.80371333],\n",
       "          [-0.10566516],\n",
       "          [ 0.97833298],\n",
       "          [ 0.99157582],\n",
       "          [ 1.10786453],\n",
       "          [ 0.74254988],\n",
       "          [-2.61420107]]),\n",
       "   array([[-0.67266147],\n",
       "          [ 0.44897091],\n",
       "          [-1.18939048],\n",
       "          [-0.36462842],\n",
       "          [-1.01565163],\n",
       "          [-0.06143737],\n",
       "          [-0.31106341],\n",
       "          [-0.35837687],\n",
       "          [-0.24746231],\n",
       "          [-0.50755417],\n",
       "          [-0.5977632 ]]),\n",
       "   array([[ 1.54153263],\n",
       "          [ 1.08911725],\n",
       "          [-0.70641458],\n",
       "          [ 0.0090992 ],\n",
       "          [ 0.51707467],\n",
       "          [ 0.27092911],\n",
       "          [ 0.29773334],\n",
       "          [ 0.2434899 ],\n",
       "          [ 0.37269916],\n",
       "          [ 1.72949191],\n",
       "          [-1.87299831]]),\n",
       "   array([[ 0.47267747],\n",
       "          [ 0.41958826],\n",
       "          [-0.59501488],\n",
       "          [ 0.53612007],\n",
       "          [-0.06751048],\n",
       "          [ 0.61318927],\n",
       "          [ 0.40247121],\n",
       "          [ 0.26931894],\n",
       "          [ 0.42492395],\n",
       "          [ 1.81220201],\n",
       "          [-1.24883169]])]),\n",
       " ('layer2',\n",
       "  [array([[  6.01157185],\n",
       "          [  7.17473396],\n",
       "          [ -6.39014812],\n",
       "          [  5.67593298],\n",
       "          [  1.00764811],\n",
       "          [-11.69979406]])])]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.model_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
